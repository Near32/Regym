extra_hyperparameters: &extra_hyperparameters
    lr_account_for_nbr_actor: False 
    weights_decay_lambda: 0.0
    weights_entropy_lambda: 0.0 #01
    use_target_to_gather_data:    False

    sequence_replay_use_zero_initial_states: False
    burn_in: True 
    sequence_replay_unroll_length: 80
    sequence_replay_overlap_length: 40
    sequence_replay_burn_in_length: 20

    sequence_replay_PER_eta: 0.9
    

LargeLSTMCNN: &LargeLSTMCNN
        phi_arch: 'CNN-LSTM-RNN' #-LSTM-RNN'
        actor_arch: 'None'
        critic_arch: 'None'
        
        # Phi Body:
        phi_arch_channels: [32, 64, 64]
        phi_arch_kernels: [8, 4, 3]
        phi_arch_strides: [4, 2, 1]
        phi_arch_paddings: [1, 1, 1]
        phi_arch_feature_dim: 512
        phi_arch_hidden_units: [512,]

        extra_inputs_infos: {
                'previous_reward':{
                    shape: [1,], 
                    target_location: ['phi_body', 'extra_inputs']
                },
                'previous_action':{
                    shape: ['task.action_dim',], 
                    target_location: ['phi_body', 'extra_inputs']
                },
                'inventory':{
                    shape: [64,], 
                    target_location: ['phi_body', 'extra_inputs']
                }
        }
        # Dictionnaries of keys living inside the 'infos' OpenAI Gym's output.
        # Value is a tuple where the first element is the expected shape of the extra input,
        # and the second item is the location where the input should be stored in the framestate.
        # Parsing of the shape will infer where to fetch the value when encountering a string.

        # Actor architecture:
        actor_arch_hidden_units: []
        # Critic architecture:
        critic_arch_hidden_units: []


r2d2_LargeLSTMCNN_obs84_graclip5m1_b32_tau1m2_lr25m5_L80_O40_B20: &r2d2_LargeLSTMCNN_obs84_graclip5m1_b32_tau1m2_lr25m5_L80_O40_B20
        dueling: False
        noisy: False 
        n_step: 1

        use_PER: False
        PER_alpha: 0.6
        PER_beta: 1.0

        replay_capacity: 1e6
        min_capacity: 1e3
        replay_period: 1

        observation_resize_dim: 64
        discount: 0.99 #0.997
        use_cuda: True
        gradient_clip: 0.5
        batch_size: 32
        tau: 1.0e-2
        learning_rate: 2.5e-4
        adam_eps: 1.0e-8

        epsstart: 1.0
        epsend: 0.01    #0.1
        epsdecay: 30000 #1000000

        saving_interval: 1e1

        <<: *LargeLSTMCNN
        <<: *extra_hyperparameters


experiment:
    tasks: [{'env-id': 'MineRLObtainDiamondVectorObf-v0', #'MineRLTreechopVectorObf-v0',
             
             'run-id': 'Seed10_venv2_LIMIT5kSteps_r2d2_Sk4_St4_Obs64_NoGrayscale_NoScaling_Replay1p6Min1p3_Demos10',
             
             'agent-id': '1step_noisy_dueling_PER_r2d2_gamma997_LargeLSTMCNN_NoGradClip_r1e6Min1e3_alpha9m1_beta4m1_tau5m3_RepP1_BURNIN_b16_L20_O10_B10',
             
             'nbr_actor': 2,
             'nbr_frame_skipping': 4,
             'nbr_frame_stacking': 4,
             'grayscale': False,
             'scaling': False,
             'observation_resize_dim': 64, #84,
             'reward_scheme': 'None', #'penalizing_progressive1e4' #'penalizing_single_reward_episode'
             # MineRL specific parameters
             'pre_train_on_demonstrations': False,
             'n_clusters': 40,
             'demo_budget': 10,
             },
            ]
    experiment_id: 'MineRL_training'
    ##benchmarking_episodes: 0
    benchmarking_episodes: 0
    ##benchmarking_interval: 1.0e8
    benchmarking_interval: 1.0e8
    benchmarking_record_episode_interval: 0
    train_observation_budget: 30000
    seed: 10


agents:    
    1step_noisy_dueling_PER_r2d2_gamma997_LargeLSTMCNN_NoGradClip_r1e6Min1e3_alpha9m1_beta4m1_tau5m3_RepP1_BURNIN_b16_L20_O10_B10:
        <<: *r2d2_LargeLSTMCNN_obs84_graclip5m1_b32_tau1m2_lr25m5_L80_O40_B20
        discount: 0.997
        gradient_clip: 0.0
        replay_capacity: 1e6
        min_capacity: 1e2
        use_PER: True
        PER_alpha: 0.9
        PER_beta: 0.4
        replay_period: 1
        batch_size: 16
        # Paper: ratio = batch_size(=32) / replay_period(=4) = 8 ,
        # but bottleneck on GPU batchsize gives a better trade-off 
        # batch-regularization-effect / speed with a batch_size=16 
        # using NVIDIA 1080 Ti... Expect ~90 it/sec, without update
        # and ~84 it/sec with updates...
        # Whereas 32 / 4 yielded ~25 it/sec....
        double: True
        dueling: True 
        noisy: True 
        n_step: 1
        ##tau: 2.5e-3
        tau: 5.0e-3
        burn_in: True
        sequence_replay_unroll_length: 20
        sequence_replay_overlap_length: 10
        sequence_replay_burn_in_length: 10

    2step_r2d2_LargeLSTMCNN_r1e5_beta4m1_tau1m3_RepP2_b16_L80_O40_B20:
        <<: *r2d2_LargeLSTMCNN_obs84_graclip5m1_b32_tau1m2_lr25m5_L80_O40_B20
        replay_capacity: 1e5
        use_PER: False
        PER_beta: 0.4
        replay_period: 2
        batch_size: 16
        # Paper: ratio = batch_size(=32) / replay_period(=4) = 8 ,
        # but bottleneck on GPU batchsize gives a better trade-off 
        # batch-regularization-effect / speed with a batch_size=16 
        # using NVIDIA 1080 Ti... Expect ~90 it/sec, without update
        # and ~84 it/sec with updates...
        # Whereas 32 / 4 yielded ~25 it/sec....
        double: True
        #dueling: True 
        #noisy: True 
        n_step: 2
        tau: 1.0e-3
