extra_hyperparameters: &extra_hyperparameters
    standardized_adv: True
    lr_account_for_nbr_actor: False 

    # BetaVAE:
    use_vae: False
    vae_weight: 1e0
    vae_nbr_latent_dim: 128
    vae_decoder_nbr_layer: 3#4
    vae_decoder_conv_dim: 128
    
    cnn_encoder_feature_dim: 128 #vae_nbr_latent_dim
    
    vae_beta: 1e2
    vae_max_capacity: 1e2
    vae_nbr_epoch_till_max_capacity: 20
    vae_constrainedEncoding: False
    vae_tc_discriminator_hidden_units: [256,256,256,256,2] #tuple([2*cnn_encoder_feature_dim']]*4+[2])
    
    # Random Network Distillation:
    use_random_network_distillation: True 
    intrinsic_discount: 0.99
    rnd_loss_int_ratio: 0.5
    rnd_obs_clip: 5
    rnd_non_episodic_int_r: True
    rnd_update_period_running_meanstd_int_reward: 1.e5
    rnd_update_period_running_meanstd_obs: 1.e5 #rnd_update_period_running_meanstd_int_reward
    # RND Convolutional Architecture:
    rnd_arch: 'CNN'
    rnd_arch_channels: [32, 64, 64]
    rnd_arch_kernels: [8, 4, 3]
    rnd_arch_strides: [4, 2, 1]
    rnd_arch_paddings: [0, 1, 1]
    rnd_arch_feature_dim: 512
    # RND Fully-Connected Architecture:
    #rnd_feature_net_fc_arch_hidden_units: (128, 64)
    rnd_weight: 1.0

LargeCNN: &LargeCNN
        phi_arch: 'CNN'
        actor_arch: 'MLP'
        critic_arch: 'MLP'
        
        # Phi Body:
        phi_arch_channels: [32, 64, 64]
        phi_arch_kernels: [8, 4, 3]
        phi_arch_strides: [4, 2, 1]
        phi_arch_paddings: [1, 1, 1]
        phi_arch_feature_dim: 512
        phi_arch_hidden_units: [512,]
        phi_arch_non_linearities: ['ReLU','ReLU', 'ReLU', 'ReLU', 'ReLU']

        # Actor architecture:
        actor_arch_hidden_units: [512, 512]
        actor_arch_non_linearities: ['ReLU', 'ReLU']
        # Critic architecture:
        critic_arch_hidden_units: [512,]
        critic_arch_non_linearities: ['ReLU']

        
ppo_LargeCNN_obs84_GAE95_V1_E1m2_graclip5m1_ep3_b256_ratio1m1_h128_lr25m5: &ppo_LargeCNN_obs84_GAE95_V1_E1m2_graclip5m1_ep3_b256_ratio1m1_h128_lr25m5
        observation_resize_dim: 84
        discount: 0.99
        use_gae: True
        use_cuda: True
        gae_tau: 0.95
        value_weight: 0.5
        entropy_weight: 0.01
        gradient_clip: 0.5
        optimization_epochs: 4 #3
        mini_batch_size: 256
        ppo_ratio_clip: 0.1
        horizon: 128
        learning_rate: 2.5e-4
        adam_eps: 1.0e-8
        
        <<: *LargeCNN
        <<: *extra_hyperparameters


experiment:
    tasks: [
            {'env-id': 'ALE/MontezumaRevenge-v5',
             
             'run-id': 'Seed13_venv_ppo_8actors_Max+Sk4_St4_Obs84_Grayscale_RandNoOpStart30',
             'agent-id': 'ppo_LargeCNN',
             
             'nbr_actor': 8,
             'nbr_frame_skipping': 1,
             'nbr_frame_stacking': 4,
             'grayscale': True,
             'single_life_episode': True,
             'nbr_max_random_steps': 0,
             'clip_reward': True,
             'observation_resize_dim': 84
             },

            #{'env-id': 'PongNoFrameskip-v4',
            # 'run-id': 'Seed13_penv_ppo_8actors_Max+Sk4_St4_Obs84_Grayscale_RandNoOpStart30',
            # 'agent-id': 'ppo_LargeCNN',
            # 
            # 'nbr_actor': 8,
            # 'nbr_frame_skipping': 4,
            # 'nbr_frame_stacking': 4,
            # 'grayscale': True,
            # 'single_life_episode': True,
            # 'nbr_max_random_steps': 30,
            # 'clip_reward': True,
            # 'observation_resize_dim': 84
            # },
            #
            #{'env-id': 'BreakoutNoFrameskip-v4',
            #
            # 'run-id': 'Seed13_penv_ppo_8actors_Max+Sk4_St4_Obs84_Grayscale_RandNoOpStart30',
            # 'agent-id': 'ppo_LargeCNN',
            # 
            # 'nbr_actor': 8,
            # 'nbr_frame_skipping': 4,
            # 'nbr_frame_stacking': 4,
            # 'grayscale': True,
            # 'single_life_episode': True,
            # 'nbr_max_random_steps': 30,
            # 'clip_reward': True,
            # 'observation_resize_dim': 84
            # },
            ]
    experiment_id: 'atari_10M_benchmark_ppo_rnd'
    benchmarking_episodes: 10
    benchmarking_interval: 5.0e5
    benchmarking_record_episode_interval: None #1.0e8
    train_observation_budget: 1.0e7
    seed: 13

agents:    
    ppo_LargeCNN:
        <<: *ppo_LargeCNN_obs84_GAE95_V1_E1m2_graclip5m1_ep3_b256_ratio1m1_h128_lr25m5
        
        rnd_nbr_frame_stacking: 4

        extra_inputs_infos: {
            #'previous_reward':{
            #    shape: [1,], 
            #    target_location: ['critic_body', 'extra_inputs']
            #},
            #'previous_action':{
            #    shape: ['task.action_dim',], 
            #    target_location: ['critic_body', 'extra_inputs']
            #},
            #'previous_action_int':{
            #    shape: [1,], 
            #    target_location: ['critic_body', 'extra_inputs']
            #},

            #'dialog':{
            #'desired_goal':{
            #    #shape: [28], #[63,], 
            #    shape: [10], #20], 
            #    target_location: ['phi_body', 'extra_inputs']
            #},
            
            # Uncomment the following if using THER_observe_achieved_goal
            #'achieved_goal':{
            #    #shape: [28], #[63,], 
            #    shape: [10], #20], 
            #    target_location: ['OracleTHER']
            #},
            
 
            #'action_mask':{
            #    shape: ['task.action_dim',], 
            #    target_location: ['critic_body', 'extra_inputs']
            #},
            #'legal_actions':{
            #    shape: ['task.action_dim',], 
            #    target_location: ['head', 'extra_inputs']
            #},

            ########################
            # WITH SAD:
            ########################
            #'greedy_action':{
            #    shape: [31,], #[6223,], 
            #    target_location: ['critic_body', 'extra_inputs']
            #},
            ########################
            ########################
               
        }

