extra_hyperparameters: &extra_hyperparameters
    lr_account_for_nbr_actor: False 
    weights_decay_lambda: 0.0 # 0.0
    weights_entropy_lambda: 0.0
    use_target_to_gather_data:    False
    
    goal_oriented: True 
    goal_state_shared_arch:  False
    goal_state_flattening: False    #True
    
    #####################################
    #####################################
    # HER Hyperparameters:
    #####################################
    nbr_training_iteration_per_cycle: 40 # HER: 40
    nbr_episode_per_cycle:  16  # HER: 16 DQN needs removal.
    HER_use_latent: False   #True
    HER_target_clamping: False 
    use_HER:    True 
    HER_strategy:   'final-1' #'future-4' #
    #####################################
    #####################################

    ####################################
    ####################################
    # R2D2 Hyperparameters:
    ####################################
    PER_compute_initial_priority: False
    #####################################
    
    sequence_replay_use_online_states: True
    sequence_replay_use_zero_initial_states: False
    sequence_replay_store_on_terminal: True
    
    r2d2_loss_masking: True
    r2d2_loss_masking_n_step_regularisation: True
    r2d2_bellman_target_SAD: False

    burn_in: True
    sequence_replay_unroll_length: 80
    sequence_replay_overlap_length: 40
    sequence_replay_burn_in_length: 20

    sequence_replay_PER_eta: 0.9
    ####################################
    vdn: False 
    vdn_nbr_players: 2
    ####################################
    ####################################


THER_extra_hyperparameters: &THER_extra_hyperparameters
    THER_use_THER:  True
    THER_use_predictor:  True
    THER_predictor_policy_shared_phi: False

    THER_max_sentence_length: 7
    THER_vocabulary: [
        'key', 'ball', 'red', 'green', 'blue', 'purple', 
        'yellow', 'grey', 'verydark', 'dark', 'neutral', 'light', 'verylight',
        'tiny', 'small', 'medium', 'large', 'giant', 'get', 'go', 'fetch', 'go', 'get',
        'a', 'fetch', 'a', 'you', 'must', 'fetch', 'a', 'to', 'the', 'box'
    ]
    
    THER_use_PER: False
    THER_PER_alpha: 0.6
    THER_PER_beta: 1.0

    THER_replay_capacity: 50000
    THER_test_replay_capacity: 500
    THER_min_capacity: 32 #1e4
    THER_replay_period: 40 # Training every episode: 40...  instead of every successfull episode... 
    #OVERRIDEN by next parameter...
    THER_train_on_success:  True
    THER_nbr_training_iteration_per_update: 40
    THER_predictor_accuracy_threshold: 0.7
    THER_predictor_test_train_split_interval: 10

    THER_predictor_learning_rate: 1e-4
    THER_predictor_batch_size: 128
    THER_gradient_clip: 10.0
    THER_weights_decay_lambda: 1.0 # 1e-6
    
LargeCNN: &LargeCNN
    sad: False 

    phi_arch: 'CNN-LSTM-RNN'
    goal_phi_arch: 'EmbedGRU'
    critic_arch: 'None'
    
    
    # Phi Body:
    phi_arch_channels: [16, M, 32, 64]
    phi_arch_kernels: [2, 2, 2, 2]
    phi_arch_strides: [1, 1, 1, 1]
    phi_arch_paddings: [1, 1, 1, 1]
    
    phi_arch_feature_dim: 64        # LSTM inputs / CNN output dim: 64
    phi_arch_hidden_units: [64,]    # LSTM hidden units: 64

    #phi_arch_feature_dim: 256 #Matching Predictor Decoder hidden size in shared arch        # LSTM inputs / CNN output dim: 64
    #phi_arch_hidden_units: [256,] #[64,]    # LSTM hidden units: 64

    # Actor architecture:
    actor_arch_hidden_units: []
    # Critic architecture:
    critic_arch_hidden_units: []

    # Goal Phi Body:
    goal_phi_arch_channels: None
    goal_phi_arch_kernels: None
    goal_phi_arch_strides: None
    goal_phi_arch_paddings: None
    goal_phi_arch_feature_dim: None
    goal_phi_arch_hidden_units: [128,]

    goal_phi_arch_embedding_size: 32

    # Critic architecture:
    goal_critic_arch_hidden_units: []

    extra_inputs_infos: {
        #'previous_reward':{
        #    shape: [1,], 
        #    target_location: ['critic_body', 'extra_inputs']
        #},
        #'previous_action':{
        #    shape: ['task.action_dim',], 
        #    target_location: ['critic_body', 'extra_inputs']
        #},

        #'action_mask':{
        #    shape: ['task.action_dim',], 
        #    target_location: ['critic_body', 'extra_inputs']
        #},
        #'legal_actions':{
        #    shape: ['task.action_dim',], 
        #    target_location: ['head', 'extra_inputs']
        #},

        ########################
        # WITH SAD:
        ########################
        #'greedy_action':{
        #    shape: ['task.action_dim',], 
        #    target_location: ['critic_body', 'extra_inputs']
        #},
        ########################
        ########################
           
    }
    
    # Dictionnaries of keys living inside the 'infos' OpenAI Gym's output.
    # Value is a tuple where the first element is the expected shape of the extra input,
    # and the second item is the location where the input should be stored in the framestate.
    # Parsing of the shape will infer where to fetch the value when encountering a string.


Predictor: &Predictor 
    predictor_encoder_arch: 'CNN'
    predictor_decoder_arch: 'CaptionGRU'
    
    # Encoder:
    predictor_encoder_arch_channels: [16, M, 32, 256]
    predictor_encoder_arch_kernels: [2, 2, 2, 2]
    predictor_encoder_arch_strides: [1, 1, 1, 1]
    predictor_encoder_arch_paddings: [1, 1, 1, 1]
    predictor_encoder_arch_feature_dim: 256        # GRU inputs / CNN output dim
    predictor_encoder_arch_hidden_units: [256,]    # GRU hidden units

    # Decoder:
    predictor_decoder_arch_channels: None
    predictor_decoder_arch_kernels: None
    predictor_decoder_arch_strides: None
    predictor_decoder_arch_paddings: None
    predictor_decoder_arch_feature_dim: None
    predictor_decoder_arch_hidden_units: [256,]

    predictor_decoder_embedding_size: 128


THER_LargeCNN_Predictor: &THER_LargeCNN_Predictor
    double: True #False
    dueling: True #False
    noisy: False 
    n_step: 3 #1

    use_PER: True #False
    PER_alpha: 0.9 #0.6
    PER_beta: 0.6 #1.0

    replay_capacity: 5242880 #50000
    min_capacity: 4.0e5 #128 #1e4
    replay_period: 1 #240 #240

    observation_resize_dim: None
    goal_resize_dim: None
    
    discount: 0.98 #0.99
    use_cuda: True
    gradient_clip: 0.5 #1.0
    batch_size: 128 #32
    tau: 4.0e-4 #1.0e-2 #THER paper:1.0e-3
    learning_rate: 6.25e-5 #1.0e-3 #1.0e-4 #1.0e-5   # 1e-4 predictor while 1e-5 network...
    adam_eps: 1.5e-5 #1.0e-8
    # NEED RMSProp optimizer...

    epsstart: 1.0
    epsend: 0.1 #0.05
    epsdecay: 10000 #500000 
    #epsdecay_strategy:   'None'

    eps_greedy_alpha: 7.0

    sequence_replay_use_online_states: True
    sequence_replay_use_zero_initial_states: False
    sequence_replay_store_on_terminal: False
    
    r2d2_loss_masking: True
    r2d2_loss_masking_n_step_regularisation: True
    
    burn_in: False
    sequence_replay_unroll_length: 40
    sequence_replay_overlap_length: 10
    sequence_replay_burn_in_length: 0

    sequence_replay_PER_eta: 0.9

    <<: *LargeCNN
    <<: *Predictor
    <<: *extra_hyperparameters
    <<: *THER_extra_hyperparameters

experiment:
    tasks: [{
        'env-id': 'BabyAI-GoToObj-v0',
        'env-confing': {},

        'run-id': 'B1M/final-1/EpPerCycle16/lrPr1m4Net1m3/Seed10_venv_Max+Sk0_St4_ObsNone_ClipReward_Eps5p5End5m2_tau100_GradClip1_THER1p1/EnvReward0p1_PredicateEps1e0_NoTargetClamping/SentL7_40MaxTrainPerUpdate_AccGoal70_THERPredTrainPeriodOnBufferedPeriodAndOnSuccess_MaxEntr1m1',
         
        'agent-id': '1step_prioritized_double_THER_LargeCNN_beta4m1_alpha_7m1_HER40TrainPerCycle_16EpPerCycle_NoPrioritizedPredTraining',
        #'agent-id': '1step_double_THER_LargeCNN_HER40TrainPerCycle_16EpPerCycle_PredPiSharedPhi',
        
        'nbr_actor': 1,
        'sad': False,
        'vdn': False,
        'otherplay': False,
        'nbr_frame_skipping': 0,
        'nbr_frame_stacking': 4,
        'single_life_episode': False,
        'nbr_max_random_steps': 0,
        'clip_reward': False,
        'observation_resize_dim': None,
        'goal_resize_dim': None,
        'reload': 'None',
    },]
    
    experiment_id: 'THER_Benchmark'
    benchmarking_episodes: 10
    benchmarking_interval: 1.0e4
    benchmarking_record_episode_interval: 1.0e8
    train_observation_budget: 1.0e7
    seed: 1

agents:    
    1step_prioritized_double_THER_LargeCNN_beta4m1_alpha_7m1_HER40TrainPerCycle_16EpPerCycle_NoPrioritizedPredTraining:
        <<: *THER_LargeCNN_Predictor
        double: True
        #dueling: True 
        #noisy: True 
        n_step: 1
        use_PER: True
        PER_alpha: 0.7
        PER_beta: 0.4
        #replay_period: 2    
        #batch_size: 16
        # Paper: ratio = batch_size(=32) / replay_period(=4) = 8 ,
        # but bottleneck on GPU batchsize gives a better trade-off 
        # batch-regularization-effect / speed with a batch_size=16 
        # using NVIDIA 1080 Ti... Expect ~90 it/sec, without update
        # and ~84 it/sec with updates...
        # Whereas 32 / 4 yielded ~25 it/sec....
        THER_use_PER: False
        THER_PER_alpha: 0.7
        THER_PER_beta: 0.4
        nbr_training_iteration_per_cycle: 40 # HER: 40
        nbr_episode_per_cycle:  16  # HER: 16
