extra_hyperparameters: &extra_hyperparameters
    lr_account_for_nbr_actor: False 
    weights_decay_lambda: 0.0
    weights_entropy_lambda: 0.0 #01
    use_target_to_gather_data:    False

    sequence_replay_use_zero_initial_states: False
    burn_in: True 
    sequence_replay_unroll_length: 80
    sequence_replay_overlap_length: 40
    sequence_replay_burn_in_length: 20

    sequence_replay_PER_eta: 0.9
    

LargeLSTMCNN: &LargeLSTMCNN
        phi_arch: 'CNN-LSTM-RNN' #-LSTM-RNN'
        actor_arch: 'None'
        critic_arch: 'None'
        
        # Phi Body:
        phi_arch_channels: [32, 64, 64]
        phi_arch_kernels: [8, 4, 3]
        phi_arch_strides: [4, 2, 1]
        phi_arch_paddings: [1, 1, 1]
        phi_arch_feature_dim: 512
        phi_arch_hidden_units: [512,]

        extra_inputs_infos: {
                'previous_reward':{
                    shape: [1,], 
                    target_location: ['phi_body', 'extra_inputs']
                },
                'previous_action':{
                    shape: ['task.action_dim',], 
                    target_location: ['phi_body', 'extra_inputs']
                },
                'inventory':{
                    shape: [64,], 
                    target_location: ['phi_body', 'extra_inputs']
                }
        }
        # Dictionnaries of keys living inside the 'infos' OpenAI Gym's output.
        # Value is a tuple where the first element is the expected shape of the extra input,
        # and the second item is the location where the input should be stored in the framestate.
        # Parsing of the shape will infer where to fetch the value when encountering a string.

        # Actor architecture:
        actor_arch_hidden_units: []
        # Critic architecture:
        critic_arch_hidden_units: []


r2d2_LargeLSTMCNN_obs84_graclip5m1_b32_tau1m2_lr25m5_L80_O40_B20: &r2d2_LargeLSTMCNN_obs84_graclip5m1_b32_tau1m2_lr25m5_L80_O40_B20
        dueling: False
        noisy: False 
        n_step: 1

        use_PER: False
        PER_alpha: 0.6
        PER_beta: 1.0

        replay_capacity: 1e6
        min_capacity: 1e3
        replay_period: 1

        observation_resize_dim: 64
        discount: 0.99 #0.997
        use_cuda: True
        gradient_clip: 0.5
        batch_size: 32
        tau: 1.0e-2
        learning_rate: 2.5e-4
        adam_eps: 1.0e-8

        epsstart: 1.0
        epsend: 0.01    #0.1
        epsdecay: 30000 #1000000

        saving_interval: 1e1

        <<: *LargeLSTMCNN
        <<: *extra_hyperparameters


experiment:
    tasks: [{'env-id': "PongNoFrameskip-v4", # MineRLTreechopVectorObf-v0', #'MineRLTreechopVectorObf-v0','MineRLObtainDiamondVectorObf-v0'
             
             'run-id': 'Seed100_venv1_r2d2_Max+Sk4_St4_Obs84_Grayscale_RandNoOpStart30_SingleLife_ClipReward_Eps3p4End1m3_EntropyReg0_WeightDecayReg0+Discount0997',
             #'Seed10_venv1_r2d2_Sk4_St4_Obs64_NoGrayscale_NoScaling_Replay1p6Min1p3_Demos1',
             
             'agent-id': '1step_dueling_PER_r2d3_gamma997_LargeLSTMCNN_GradClip4p1_r1e4Min1e3_alpha9m1_beta4m1_tau2m1_RepP4_BURNIN_b4_L8_O4_B4_demoR0',
             
             'nbr_actor': 1, #4,
             'nbr_frame_skipping': 4,
             'nbr_frame_stacking': 4,
             'grayscale': True,
             'single_life_episode': True,
             'nbr_max_random_steps': 30,
             'clip_reward': True,
             'previous_reward_action': True,
             'observation_resize_dim': (84,84),

             'reward_scheme': 'None', #'penalizing_progressive1e4' #'penalizing_single_reward_episode'
             
             #MineRL specific parameters
             'pre_train_on_demonstrations': False,
             'n_clusters': 40,
             'demo_budget': 1,
             },
            ]
    experiment_id: 'r2d3_debug' #'MineRL_training'
    ##benchmarking_episodes: 0
    benchmarking_episodes: 0
    ##benchmarking_interval: 1.0e8
    benchmarking_interval: 1.0e8
    benchmarking_record_episode_interval: 0
    train_observation_budget: 5e6
    seed: 100


agents:
    1step_noisy_PER_dueling_r2d2_gamma997_LargeLSTMCNN_GradClip4p1_r1e4Min1e3_alpha9m1_beta4m1_over1e4_eta9m1_tau1m3_RepP4_BURNIN_b8_L8_O4_B4:
        <<: *r2d2_LargeLSTMCNN_obs84_graclip5m1_b32_tau1m2_lr25m5_L80_O40_B20
        discount: 0.997
        gradient_clip: 40.0
        replay_capacity: 1e4
        min_capacity: 1e3
        use_PER: True 
        PER_alpha: 0.9
        PER_beta: 0.4
        PER_beta_increase_interval: 1e4
        sequence_replay_PER_eta: 0.9
        replay_period: 4
        batch_size: 8
        # Paper: ratio = batch_size(=32) / replay_period(=4) = 8 ,
        # but bottleneck on GPU batchsize gives a better trade-off 
        # batch-regularization-effect / speed with a batch_size=16 
        # using NVIDIA 1080 Ti... Expect ~90 it/sec, without update
        # and ~84 it/sec with updates...
        # Whereas 32 / 4 yielded ~25 it/sec....
        double: True
        dueling: True 
        noisy: True
        n_step: 1
        tau: 1.0e-3 #2.5e-3
        burn_in: True
        sequence_replay_unroll_length: 8
        sequence_replay_overlap_length: 4
        sequence_replay_burn_in_length: 4

    1step_dueling_PER_r2d3_gamma997_LargeLSTMCNN_GradClip4p1_r1e4Min1e3_alpha9m1_beta4m1_tau2m1_RepP4_BURNIN_b4_L8_O4_B4_demoR0:
        <<: *r2d2_LargeLSTMCNN_obs84_graclip5m1_b32_tau1m2_lr25m5_L80_O40_B20
        discount: 0.997
        gradient_clip: 40.0
        replay_capacity: 1e4
        min_capacity: 1e3
        use_PER: True
        PER_alpha: 0.9
        PER_beta: 0.4
        replay_period: 4
        batch_size: 4
        double: True
        dueling: True 
        noisy: False
        n_step: 1
        tau: 1.0e-3
        burn_in: True
        sequence_replay_unroll_length: 8
        sequence_replay_overlap_length: 4
        sequence_replay_burn_in_length: 4
        demo_ratio: 0.0
        minerl: False
        demo_skip: 4
        demo_stack: 4
        # only useful for MineRL:
        demo_budget: 20
        # path to the agent whose replay buffer we load in:
        expert_buffer_path: ""
